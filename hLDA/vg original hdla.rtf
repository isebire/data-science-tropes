{\rtf1\ansi\ansicpg1252\cocoartf2578
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fswiss\fcharset0 Helvetica;\f1\fnil\fcharset0 Menlo-Regular;}
{\colortbl;\red255\green255\blue255;}
{\*\expandedcolortbl;;}
\paperw11900\paperh16840\margl1440\margr1440\vieww11520\viewh8400\viewkind0
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0\fs24 \cf0 vg original hlda results\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0

\f1\fs26 \cf0 \CocoaLigature0 Number of live topics: 10\
Analysing at level 0\
Topics at level:\
[0]\
Topic0\
[9 8]\
-1\
Analysing at level 1\
Topics at level:\
[8, 9]\
Topic8\
[22 24 26 28 19 16]\
0\
Topic9\
[20]\
0\
Analysing at level 2\
Topics at level:\
[16, 19, 20, 22, 24, 26, 28]\
Topic16\
[]\
8\
Topic19\
[]\
8\
Topic20\
[]\
9\
Topic22\
[]\
8\
Topic24\
[]\
8\
Topic26\
[]\
8\
Topic28\
[]\
8\
<Basic Info>\
| HLDAModel (current version: 0.12.2)\
| 8653 docs, 1066418 words\
| Total Vocabs: 26950, Used Vocabs: 26950\
| Entropy of words: 9.16221\
| Entropy of term-weighted words: 9.16221\
| Removed Vocabs: <NA>\
|\
<Training Info>\
| Iterations: 1100, Burn-in steps: 0\
| Optimization Interval: 10\
| Log-likelihood per word: -9.46798\
|\
<Initial Parameters>\
| tw: TermWeight.ONE\
| min_cf: 0 (minimum collection frequency of words)\
| min_df: 0 (minimum document frequency of words)\
| rm_top: 0 (the number of top words to be removed)\
| depth: 3 (the maximum depth level of hierarchy between 2 ~ 32767)\
| alpha: [0.1] (hyperparameter of Dirichlet distribution for document-depth level, given as a single `float` in case of symmetric prior and as a list with length `depth` of `float` in case of asymmetric prior.)\
| eta: 0.01 (hyperparameter of Dirichlet distribution for topic-word)\
| gamma: 0.1 (concentration coeficient of Dirichlet Process)\
| seed: 3975801642 (random seed)\
| trained in version 0.12.2\
|\
<Parameters>\
| alpha (Dirichlet prior on the per-document depth level distributions)\
|  [0.7575663 1.132568  1.2971487]\
| eta (Dirichlet prior on the per-topic word distribution)\
|  0.01\
| gamma (concentration coeficient of Dirichlet Process)\
|  0.1\
| Number of Topics: 10\
|\
<Topics>\
| #0 (347699, 8653) : fantasyvideogames japanesegames optionalboss easternrpg infinityplusonesword\
|   #8 (349015, 8650) : shoutout nintendohard finalboss videogamesofthe1990s bigbad\
|     #16 (344248, 6229) : shoutout videogamesof20152019 foreshadowing bigbad multipleendings\
|     #19 (140, 5) : worksneedingtropes historicaldomaincharacter excitedshowtitle uselessitem stylishaction\
|     #22 (112, 5) : realtimewithpause buxomisbetter myeyesareuphere twomenonedress mockmillionaire\
|     #24 (205, 15) : allegedlyfreegame mobilephonegame premiumcurrency videogamesof20102014 bribingyourwaytovictory\
|     #26 (17645, 1740) : videogamesof20152019 videogamesofthe1980s platformgame videogamesofthe1990s onehitpointwonder\
|     #28 (6288, 656) : japanesegames videogamesof20152019 videogamesof20102014 fightinggame videogamesofthe1990s\
|   #9 (545, 3) : enforcedtechnologylevels flashstep storytogameplayratio ghostship abusiveprecursors\
|     #20 (521, 3) : armorpiercingattack beamspam pacmanfever geniusloci fakebalance\
|\
\
[0.52814347 0.17768684 0.29416966]}